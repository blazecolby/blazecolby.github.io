{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "title: Multi Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "Data Overview  \n",
    "\n",
    "Ames, Iowa: Alternative to the Boston Housing Data ( 2006 to 2010 )\n",
    "\n",
    "- 2930 observations (Property Sales)  \n",
    "- Explanatory variables.  \n",
    "- 23 nominal - mainly dwelling structures.  \n",
    "- 23 ordinal - rate various items in property.  \n",
    "- 14 discrete - number of items; kitchens, bathrooms.  \n",
    "- 20 continuous - typically are dimensions.  \n",
    "\n",
    "NOTE: Remove houses > 4000 sqft.\n",
    "\n",
    "We want to view housing data with different regression analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas_profiling\n",
    "import matplotlib as lib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pydataset import data\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test and train set\n",
    "test = pd.read_csv('Ames Housing Data/test.csv')\n",
    "train = pd.read_csv('Ames Housing Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View shape of data\n",
    "print('Shape of test set', test.shape)\n",
    "print('Shape of train set', train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis\n",
    "\n",
    "Housing price distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.rcParams['figure.facecolor']= 'black'\n",
    "lib.rcParams['axes.facecolor']= 'black'\n",
    "lib.rcParams['lines.markersize'] = 10\n",
    "lib.rcParams[\"scatter.marker\"] = '.'\n",
    "lib.rcParams['figure.titlesize']= 100\n",
    "lib.rcParams['figure.figsize']=(18, 5)\n",
    "plt.xticks(color='w')\n",
    "plt.xlabel('Price', color='w')\n",
    "plt.ylabel('Ratio', color='w')\n",
    "plt.yticks(color='w')\n",
    "plt.figtext(.5,.9,'Housing Price Distribution', fontsize=20, ha='center', color='w')\n",
    "\n",
    "ax = sns.distplot(train.SalePrice, rug=True,\n",
    "                  rug_kws={\"color\": \"darkgrey\", \"lw\": .3, \"height\":.1, 'alpha':1},\n",
    "                  kde_kws={\"color\": \"r\", \"lw\": 1, \"label\": \"KDE\"},\n",
    "                  hist_kws={\"histtype\": \"step\", \"linewidth\": 1, \"alpha\":1, \"color\": \"r\"})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and Kurtosis  \n",
    " \n",
    "print(\"Skewness: \", train['SalePrice'].skew(), '| Biased towards the right due to a few high outliers.')\n",
    "print(\"Kurtosis: \", train['SalePrice'].kurt(), '| Sharpness of peak, normal dist = 3')\n",
    "print('Average House Price: ', round(train['SalePrice'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review price to sqft to find outliers.\n",
    " \n",
    "lib.rcParams['figure.facecolor']= 'black'\n",
    "lib.rcParams['axes.facecolor']= 'black'\n",
    "lib.rcParams['lines.markersize'] = 10\n",
    "lib.rcParams[\"scatter.marker\"] = '.'\n",
    "lib.rcParams['figure.titlesize']= 100\n",
    "lib.rcParams['figure.figsize']=(15, 10)\n",
    "plt.xticks(color='w')\n",
    "plt.xlabel('Price', color='w')\n",
    "plt.ylabel('Squarefoot', color='w')\n",
    "plt.yticks(color='w')\n",
    "\n",
    "sns.scatterplot(data=train, x='SalePrice', y='GrLivArea')\n",
    "plt.figtext(.5,.9,'Housing | Price Vs. Sqft', fontsize=20, ha='center', color='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "By removing the 4 outliers, we're now able to bring our data closer to a normal distribution.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['GrLivArea'] < 4000]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness and Kurtosis - 2nd check.\n",
    " \n",
    "print(\"Skewness: \", train['SalePrice'].skew(), '| Biased towards the right due to a few high outliers.')\n",
    "print(\"Kurtosis: \", train['SalePrice'].kurt(), '| Sharpness of peak, normal dist = 3')\n",
    "print('Average House Price: ', round(train['SalePrice'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "lib.rcParams['figure.facecolor'] = 'w'\n",
    "\n",
    "corrmat = train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive correlation distribution\n",
    " \n",
    "c = train.corr()\n",
    "s = c.unstack()\n",
    "positive_corr = s.sort_values(kind=\"quicksort\", ascending=False)\n",
    "negative_corr = s.sort_values(kind=\"quicksort\")\n",
    "\n",
    "positive_corr = positive_corr[positive_corr != 1]\n",
    "negative_corr = negative_corr[negative_corr != 1]\n",
    "\n",
    "lib.rcParams['figure.facecolor']= 'black'\n",
    "lib.rcParams['axes.facecolor']= 'black'\n",
    "lib.rcParams['lines.markersize'] = 10\n",
    "lib.rcParams[\"scatter.marker\"] = '.'\n",
    "lib.rcParams['figure.titlesize']= 100\n",
    "lib.rcParams['figure.figsize']=(24, 9)\n",
    "plt.xticks(color='w')\n",
    "plt.yticks(color='w')\n",
    "\n",
    "plt.xlabel('Correlation', color='w')\n",
    "plt.ylabel('Distribution', color='w')\n",
    "sns.distplot(positive_corr)\n",
    "plt.figtext(.5,.9,'Positive Correlation Distribution', fontsize=20, ha='center', color='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative correlation ditribution\n",
    " \n",
    "lib.rcParams['figure.facecolor']= 'black'\n",
    "lib.rcParams['axes.facecolor']= 'black'\n",
    "lib.rcParams['lines.markersize'] = 10\n",
    "lib.rcParams[\"scatter.marker\"] = '.'\n",
    "lib.rcParams['figure.titlesize']= 100\n",
    "lib.rcParams['figure.figsize']=(24, 9)\n",
    "plt.xticks(color='w')\n",
    "plt.yticks(color='w')\n",
    "\n",
    "plt.xlabel('Correlation', color='w')\n",
    "plt.ylabel('Distribution', color='w')\n",
    "sns.distplot(negative_corr)\n",
    "plt.figtext(.5,.9,'Negative Correlation Distribution', fontsize=20, ha='center', color='w')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean, transform, and encode data.\n",
    " \n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',\n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',\n",
    "        'YrSold', 'MoSold')\n",
    "\n",
    "# Nans = no feature\n",
    "feature_does_not_exist = [ 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n",
    "                           'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageFinish', 'GarageQual', 'GarageCond',\n",
    "                           'PoolQC', 'Fence', 'MiscFeature' ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    " \n",
    "train.Alley = train.Alley.fillna(\"No access\")\n",
    "\n",
    "# Fill in select Nans\n",
    "for x in feature_does_not_exist:\n",
    "    train[x] = train[x].fillna('None')\n",
    "\n",
    "train['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\n",
    "\n",
    "# Turn into categorical information\n",
    "train['MSSubClass'] = train['MSSubClass'].apply(str)\n",
    "train['OverallCond'] = train['OverallCond'].astype(str)\n",
    "train['YrSold'] = train['YrSold'].astype(str)\n",
    "train['MoSold'] = train['MoSold'].astype(str)\n",
    "\n",
    "# Labelencode\n",
    "for x in cols:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(pd.Series(train[x].values)))\n",
    "    train[x] = lbl.transform(list(pd.Series(train[x].values)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness\n",
    " \n",
    "numeric = train.dtypes[train.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = train[numeric].apply(lambda x: pd.DataFrame.skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Skewness\n",
    " \n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    train[feat] = boxcox1p(train[feat], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "\n",
    "train[train==np.inf]=np.nan\n",
    "train.fillna(train.mean(), inplace=True)\n",
    "\n",
    "X_train = train\n",
    "y_train = train.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "\n",
    "# Kfold function randomizes our data, and splits it into train/test sets.  \n",
    "# After kfold -- run cross valiation. This runs our given model on the number of folds that we have specified.  \n",
    "# This allows us to run a test on each section of our training data.  \n",
    " \n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression\n",
    " \n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "\n",
    "# Linear regression with combined L1 and L2 as regularizer.\n",
    " \n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel Ridge Score\n",
    " \n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost Regression\n",
    " \n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10,\n",
    "                                   loss='huber', random_state=5)\n",
    "\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG Boost Regression\n",
    " \n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,\n",
    "                             learning_rate=0.05, max_depth=3,\n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state=7, nthread = -1)\n",
    "\n",
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM Regression\n",
    " \n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf=6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of models\n",
    " \n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run class\n",
    " \n",
    "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std())) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
