{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Compenents Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a technique that reduces the number of variables in a dataset down to a smaller set of variables in which the majority of information from all variables is retained in the smaller set, in turn taking a set of correlated variables and turning them into a smaller set of variables that in uncorrelated. \n",
    "\n",
    "PCA finds variables that have a shared variance, it then creates a new variable that represents that shared variance. When this happens some of the initial variance is lost. This is important to keep in mind. If we are creating an explainable model then we may want to think about what the exact variance is that is being left out. In the same sense, reduction techniques may be useful to the end user by only showing the 3 most important things to focus on. Less features also may lead to less overfitting. The goal is to explain the maximum amount of variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets as ds\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = ds.load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Descriptions: </br>\n",
    "- CRIM    - per capita crime rate by town </br>\n",
    "- ZN      - proportion of residential land zoned for lots over 25,000 sq.ft.</br>\n",
    "- INDUS   - proportion of non-retail business acres per town.</br>\n",
    "- CHAS    - Charles River dummy variable (1 if tract bounds river; 0 otherwise)</br>\n",
    "- NOX     - nitric oxides concentration (parts per 10 million)</br>\n",
    "- RM      - average number of rooms per dwelling</br>\n",
    "- AGE     - proportion of owner-occupied units built prior to 1940</br>\n",
    "- DIS     - weighted distances to five Boston employment centres</br>\n",
    "- RAD     - index of accessibility to radial highways</br>\n",
    "- TAX     - full-value property-tax rate per 10_000</br>\n",
    "- PTRATIO - pupil-teacher ratio by town </br>\n",
    "- B       - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town </br>\n",
    "- LSTAT   - Percent lower status of the population </br>\n",
    "- MEDV    - Median value of owner-occupied homes in 1_000's</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston dataset can be used to predict two prototasks. One is nitrous oxide levels and the other is price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a parametric model, meaning it works best under the statistical assumptions of a normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a preview of PCA using sklearn. We can see that the explained variance drops off fairly quickly for the number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80582318 0.16305197 0.02134861 0.00695699 0.00129995]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca.fit(X);\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we switch to 2 variables to keep the majority of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80582318 0.16305197]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X);\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a partial correlation matrix between:\n",
    "CRIM(per capita crime rate by town) & ZN(proportion of res. land zoned for lots over 25,000 sq.ft)\n",
    "&\n",
    "CRIM(per capita crime rate by town) & ZNINDUS(proportion of non-retail business acres per town)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>CRIM</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ZN</td>\n",
       "      <td>-0.200469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>INDUS</td>\n",
       "      <td>0.406583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CRIM\n",
       "CRIM   1.000000\n",
       "ZN    -0.200469\n",
       "INDUS  0.406583"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.corr(method='pearson').iloc[:3,:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix is a covariance matrix where the covariances have been divided by the variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$var(x)=\\frac{\\sum(x_i-\\bar{x})^2}n$$\n",
    "\n",
    "$$cov(A)=\\sum\\frac{(x_i-\\bar{x})(y_i-\\bar{y})}n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two rows of a covariance matrix: \n",
      " \n",
      " [[ 1.0019802  -0.20086619  0.40738853 -0.05600226  0.42180532 -0.21968085\n",
      "   0.35343273 -0.38042191  0.62674377  0.5839183   0.29051973 -0.38582644\n",
      "   0.4565237 ]\n",
      " [-0.20086619  1.0019802  -0.53488527 -0.04278127 -0.51762669  0.31260839\n",
      "  -0.57066514  0.66572388 -0.31256554 -0.31518622 -0.39245415  0.17586788\n",
      "  -0.41381239]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# StandardScaler = (x - mean) / std == the mean and std of each variable\n",
    "# fit_transform = standardization by centering and scaling\n",
    "\n",
    "#  All variables have a mean of 0 and std of 1\n",
    "x = StandardScaler().fit_transform(X)\n",
    "xt = x.T # numpty thinks variables are rows\n",
    "Cx = np.cov(xt) \n",
    "print('First two rows of a covariance matrix: \\n \\n', Cx[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in PCA is to center the data of matrix X. In this case our matrix X is the dataframe X with n number of features. Centering is done by subtracting the mean of the whole of the data from each data point. which helps remove bias.\n",
    "\n",
    "Calculate the covariance matrix. Then apply a linear transformation. Next derive the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EigenVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwhiten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msvd_solver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0miterated_power\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Principal component analysis (PCA)\n",
       "\n",
       "Linear dimensionality reduction using Singular Value Decomposition of the\n",
       "data to project it to a lower dimensional space. The input data is centered\n",
       "but not scaled for each feature before applying the SVD.\n",
       "\n",
       "It uses the LAPACK implementation of the full SVD or a randomized truncated\n",
       "SVD by the method of Halko et al. 2009, depending on the shape of the input\n",
       "data and the number of components to extract.\n",
       "\n",
       "It can also use the scipy.sparse.linalg ARPACK implementation of the\n",
       "truncated SVD.\n",
       "\n",
       "Notice that this class does not support sparse input. See\n",
       ":class:`TruncatedSVD` for an alternative with sparse data.\n",
       "\n",
       "Read more in the :ref:`User Guide <PCA>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_components : int, float, None or string\n",
       "    Number of components to keep.\n",
       "    if n_components is not set all components are kept::\n",
       "\n",
       "        n_components == min(n_samples, n_features)\n",
       "\n",
       "    If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n",
       "    MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n",
       "    will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n",
       "\n",
       "    If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n",
       "    number of components such that the amount of variance that needs to be\n",
       "    explained is greater than the percentage specified by n_components.\n",
       "\n",
       "    If ``svd_solver == 'arpack'``, the number of components must be\n",
       "    strictly less than the minimum of n_features and n_samples.\n",
       "\n",
       "    Hence, the None case results in::\n",
       "\n",
       "        n_components == min(n_samples, n_features) - 1\n",
       "\n",
       "copy : bool (default True)\n",
       "    If False, data passed to fit are overwritten and running\n",
       "    fit(X).transform(X) will not yield the expected results,\n",
       "    use fit_transform(X) instead.\n",
       "\n",
       "whiten : bool, optional (default False)\n",
       "    When True (False by default) the `components_` vectors are multiplied\n",
       "    by the square root of n_samples and then divided by the singular values\n",
       "    to ensure uncorrelated outputs with unit component-wise variances.\n",
       "\n",
       "    Whitening will remove some information from the transformed signal\n",
       "    (the relative variance scales of the components) but can sometime\n",
       "    improve the predictive accuracy of the downstream estimators by\n",
       "    making their data respect some hard-wired assumptions.\n",
       "\n",
       "svd_solver : string {'auto', 'full', 'arpack', 'randomized'}\n",
       "    auto :\n",
       "        the solver is selected by a default policy based on `X.shape` and\n",
       "        `n_components`: if the input data is larger than 500x500 and the\n",
       "        number of components to extract is lower than 80% of the smallest\n",
       "        dimension of the data, then the more efficient 'randomized'\n",
       "        method is enabled. Otherwise the exact full SVD is computed and\n",
       "        optionally truncated afterwards.\n",
       "    full :\n",
       "        run exact full SVD calling the standard LAPACK solver via\n",
       "        `scipy.linalg.svd` and select the components by postprocessing\n",
       "    arpack :\n",
       "        run SVD truncated to n_components calling ARPACK solver via\n",
       "        `scipy.sparse.linalg.svds`. It requires strictly\n",
       "        0 < n_components < min(X.shape)\n",
       "    randomized :\n",
       "        run randomized SVD by the method of Halko et al.\n",
       "\n",
       "    .. versionadded:: 0.18.0\n",
       "\n",
       "tol : float >= 0, optional (default .0)\n",
       "    Tolerance for singular values computed by svd_solver == 'arpack'.\n",
       "\n",
       "    .. versionadded:: 0.18.0\n",
       "\n",
       "iterated_power : int >= 0, or 'auto', (default 'auto')\n",
       "    Number of iterations for the power method computed by\n",
       "    svd_solver == 'randomized'.\n",
       "\n",
       "    .. versionadded:: 0.18.0\n",
       "\n",
       "random_state : int, RandomState instance or None, optional (default None)\n",
       "    If int, random_state is the seed used by the random number generator;\n",
       "    If RandomState instance, random_state is the random number generator;\n",
       "    If None, the random number generator is the RandomState instance used\n",
       "    by `np.random`. Used when ``svd_solver`` == 'arpack' or 'randomized'.\n",
       "\n",
       "    .. versionadded:: 0.18.0\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "components_ : array, shape (n_components, n_features)\n",
       "    Principal axes in feature space, representing the directions of\n",
       "    maximum variance in the data. The components are sorted by\n",
       "    ``explained_variance_``.\n",
       "\n",
       "explained_variance_ : array, shape (n_components,)\n",
       "    The amount of variance explained by each of the selected components.\n",
       "\n",
       "    Equal to n_components largest eigenvalues\n",
       "    of the covariance matrix of X.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "explained_variance_ratio_ : array, shape (n_components,)\n",
       "    Percentage of variance explained by each of the selected components.\n",
       "\n",
       "    If ``n_components`` is not set then all components are stored and the\n",
       "    sum of the ratios is equal to 1.0.\n",
       "\n",
       "singular_values_ : array, shape (n_components,)\n",
       "    The singular values corresponding to each of the selected components.\n",
       "    The singular values are equal to the 2-norms of the ``n_components``\n",
       "    variables in the lower-dimensional space.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "mean_ : array, shape (n_features,)\n",
       "    Per-feature empirical mean, estimated from the training set.\n",
       "\n",
       "    Equal to `X.mean(axis=0)`.\n",
       "\n",
       "n_components_ : int\n",
       "    The estimated number of components. When n_components is set\n",
       "    to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n",
       "    number is estimated from input data. Otherwise it equals the parameter\n",
       "    n_components, or the lesser value of n_features and n_samples\n",
       "    if n_components is None.\n",
       "\n",
       "noise_variance_ : float\n",
       "    The estimated noise covariance following the Probabilistic PCA model\n",
       "    from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
       "    Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
       "    http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n",
       "    compute the estimated data covariance and score samples.\n",
       "\n",
       "    Equal to the average of (min(n_features, n_samples) - n_components)\n",
       "    smallest eigenvalues of the covariance matrix of X.\n",
       "\n",
       "References\n",
       "----------\n",
       "For n_components == 'mle', this class uses the method of *Minka, T. P.\n",
       "\"Automatic choice of dimensionality for PCA\". In NIPS, pp. 598-604*\n",
       "\n",
       "Implements the probabilistic PCA model from:\n",
       "Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n",
       "component analysis\". Journal of the Royal Statistical Society:\n",
       "Series B (Statistical Methodology), 61(3), 611-622.\n",
       "via the score and score_samples methods.\n",
       "See http://www.miketipping.com/papers/met-mppca.pdf\n",
       "\n",
       "For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n",
       "\n",
       "For svd_solver == 'randomized', see:\n",
       "*Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n",
       "\"Finding structure with randomness: Probabilistic algorithms for\n",
       "constructing approximate matrix decompositions\".\n",
       "SIAM review, 53(2), 217-288.* and also\n",
       "*Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n",
       "\"A randomized algorithm for the decomposition of matrices\".\n",
       "Applied and Computational Harmonic Analysis, 30(1), 47-68.*\n",
       "\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.decomposition import PCA\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
       ">>> pca = PCA(n_components=2)\n",
       ">>> pca.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n",
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)\n",
       ">>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n",
       "[0.9924... 0.0075...]\n",
       ">>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n",
       "[6.30061... 0.54980...]\n",
       "\n",
       ">>> pca = PCA(n_components=2, svd_solver='full')\n",
       ">>> pca.fit(X)                 # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='full', tol=0.0, whiten=False)\n",
       ">>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n",
       "[0.9924... 0.00755...]\n",
       ">>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n",
       "[6.30061... 0.54980...]\n",
       "\n",
       ">>> pca = PCA(n_components=1, svd_solver='arpack')\n",
       ">>> pca.fit(X)  # doctest: +NORMALIZE_WHITESPACE\n",
       "PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
       "  svd_solver='arpack', tol=0.0, whiten=False)\n",
       ">>> print(pca.explained_variance_ratio_)  # doctest: +ELLIPSIS\n",
       "[0.99244...]\n",
       ">>> print(pca.singular_values_)  # doctest: +ELLIPSIS\n",
       "[6.30061...]\n",
       "\n",
       "See also\n",
       "--------\n",
       "KernelPCA\n",
       "SparsePCA\n",
       "TruncatedSVD\n",
       "IncrementalPCA\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\blaze.rogers\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\decomposition\\pca.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "This particular implementation uses Singular Value Decomposition (SVD).\n",
    "The input data is centered but not scaled for each feature before applying the SVD.\n",
    "\n",
    "It uses the LAPACK implementation of the full SVD or a randomized truncated\n",
    "SVD by the method of Halko et al. 2009, depending on the shape of the input\n",
    "data and the number of components to extract.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
