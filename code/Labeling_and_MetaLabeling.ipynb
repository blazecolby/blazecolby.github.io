{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Explainability} $$\n",
    "-- \n",
    "$$ \\text{--} $$\n",
    "$$ \\text{Trend Following Strategy Implementation} $$\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will explore explainability through the use of a Trend-Following strategy by using a Simple Moving Average(SMA) Cross Over Strategy with Meta-Labeling.\n",
    "\n",
    "Marcos Lopez de Prado's approach to explainability is woven throughout his overall application of ML in finance. A few examples include the ability to understand data through proper labeling(dollar bars, tick bars), the ability to utilize inferential analysis by maximizing the amount of information retained in traditional time-series models like ARIMAs by using items such as fractional differentiation(), or the ability to come up with strategies separate from models so we're not relying on ML models to generate betting opportunities - instead only the ability to determine whether to pass on a bet or not. Further understanding/conviction of how he applies explainability can be built up from digging into his examples. \n",
    " \n",
    "One topic that I wanted to cover as a feature of explainability from Prado's book is 'Meta-Labeling', which is essentially a black box model on top of a white box. We will jump into Meta-Labeling below.\n",
    "\n",
    "\n",
    "The majority of the functions were pulled from 'Advances in Financial Machine Learning' written by Marcos Lopez de Prado.<br>\n",
    "Keep in mind that when working with time series data, the general tendency used here is to measure items in fixed chunks of time where we can apply statistical assumptions and use better models/more accurate inferences.<br>\n",
    "\n",
    "Explainations of preceeding functions:\n",
    "___\n",
    "TRIPLE BARRIER METHOD & META-LABELING: \n",
    "\n",
    "The triple barrier method is used to label a fixed time horizon series. Labeling is an important aspect in explainability. It allows us to ensure that we are maximizing the understanding of our data before we place it in a model. Triple-Barrier-Method labels observations based on the first barrier touched,  then gives the target that was used to generate the horizontal barrier. Two horizontal barriers, profit taking(above) and stop-loss(below) limits; one vertical barrier(right side), time lapse in days or number of bars passed since first position. This will return one of three scenarios, which every barrier is touched first:\n",
    "- Upper limit: 1\n",
    "- Lower limit: -1\n",
    "- Vertical limit: 0\n",
    "\n",
    "Meta-Labeling separates the sides(Triple-Barrier) from the size. When our triple-barrier method's side is not None then we can use Meta-Labeling. Meta-Labeling discriminates between our profits and stop-loss orders. Now instead of getting a {1,0,-1} we get {1,0}. The ML algorithm will be trained to bet or pass. Then we can this primary model's result(signs) and use a secondary model to find the probabilities to decide on the size of the bet. Since the primary model is now binary we can filter out false positives and false negatives, and use a ROC curve to view the cost of increasing the true positive rate of a bet. First, we build a model with high recall. Then we correct the low precision by using met-labeling to the positives predicted by the model. In this we make our model more granular and can more readily ascertain why our model is making certain decisions. \n",
    "\n",
    "\n",
    "EVENT-BASED SAMPLINGS<br>\n",
    "\\- getTEvents: Detect shift in mean value of a measured quantity away from a target value.<br>\n",
    "COMPUTING DYNAMIC THRESHOLDS<br>\n",
    "\\- getDailyVol:  set profit taking and stoploss limits that are a function of the risks involved in a bet<br>\n",
    "TRIPLE-BARRIER METHOD<br>\n",
    "\\- applyPtSlOnT1: label an observation according to the first barrier touched out of three barriers<br>\n",
    "LEARNING SIDE AND SIZE<br>\n",
    "\\- getEvents: learn sides(to go long or short on a bet), learn size(how much to risk)<br>\n",
    "ADDING A VERTICAL BARRIER<br>\n",
    "\\- addVerticalBarrier: For each index in tEvents, get timestamp of next price bar at or immediately after a number of days<br>\n",
    "LABELING FOR SIDE AND SIZE<br>\n",
    "\\- getBins: Label observations for return at time of first barrier touched, and {-1,0,1}<br>\n",
    "DROPPING UNDER-POPULATED LABELS<br>\n",
    "\\- dropLabels: drop observations less than significant % of total classes, ie. 5%\n",
    "___\n",
    "LABELS:\n",
    "\n",
    "STANDARD BARS<br>\n",
    "\\- general_bars: generates base function for standard bars to create dollar bars and tick bars.<br>\n",
    "STANDARD BARS DF<br>\n",
    "\\- general_bar_df: turns general_bars to dataframe<br>\n",
    "DOLLAR BARS<br>\n",
    "\\- dollar_bar_df: formed by sampling an observation every time a pre-defined market value is exchanged.<br>\n",
    "TICK BARS<br>\n",
    "\\- tick_bars: extracted each time a pre-defined number of transactions takes place.\n",
    "\n",
    "___\n",
    "Can mainly ignore this.. didn't have time to fully go through the functions to simplify them. \n",
    "\n",
    "PROCESSING SPEEDS:\n",
    "\n",
    "ATOMS AND MOLECULES - Atoms are indivisible tasks i.e. single threaded, molecules are grouped i.e. parallel processing. \n",
    "\n",
    "LINEAR PARTITIONS<br>\n",
    "\\- linParts: Create list of atoms in subsets of equal size, proportional to number of processors. <br>\n",
    "TWO-NESTED LOOPS PARTITIONS<br>\n",
    "\\- nestedParts: parallelize nested functions<br>\n",
    "MULTIPROCESSING ENGINE<br>\n",
    "\\- mpPandasObj: parallelization wrapper<br>\n",
    "SINGLE-THREAD EXECUTION, FOR DEBUGGING<br>\n",
    "\\- processJobs_: runs jobs sequentially<br>\n",
    "CENTAGE OF JOBS COMPLETED<br>\n",
    "\\- reportProgress: shows progress for number of asynchronous jobs.<br>\n",
    "ASYNCHRONOUS CALL<br>\n",
    "\\- processJobs: run jobs in parallel, and process back into single output if needed. <br>\n",
    "UNWRAPPING THE CALLBACK<br>\n",
    "\\- expandCall: executes each job(each thread). Turns dictionary into task. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Import libraries and create functions:\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUkBfZ4B1RR6",
    "outputId": "98b059f0-2b83-4bea-955e-daaa474b3336"
   },
   "outputs": [],
   "source": [
    "import re, os, sys, time, types #!pip install pyarrow\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd # Don't use pandas 1.0.0\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPYEue-P1RSL"
   },
   "outputs": [],
   "source": [
    "## Symmetric CUSUM Filter\n",
    "def getTEvents(gRaw, h):\n",
    "    '''\n",
    "    Symmetric CUSUM Filter\n",
    "    Sample a bar t iff S_t >= h at which point S_t is reset\n",
    "    Multiple events are not triggered by gRaw hovering around a threshold level\n",
    "    It will require a full run of length h for gRaw to trigger an event\n",
    "    gRaw: the raw time series we wish to filter (gRaw)\n",
    "    h: threshold\n",
    "    Return: pd.DatatimeIndex.append(tEvents)\n",
    "    '''\n",
    "    tEvents, sPos, sNeg = [], 0, 0\n",
    "    diff = np.log(gRaw).diff().dropna()\n",
    "    for i in tqdm(diff.index[1:]):\n",
    "        try:\n",
    "            pos, neg = float(sPos+diff.loc[i]), float(sNeg+diff.loc[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(sPos+diff.loc[i], type(sPos+diff.loc[i]))\n",
    "            print(sNeg+diff.loc[i], type(sNeg+diff.loc[i]))\n",
    "            break\n",
    "        sPos, sNeg=max(0., pos), min(0., neg)\n",
    "        if sNeg<-h:\n",
    "            sNeg=0;tEvents.append(i)\n",
    "        elif sPos>h:\n",
    "            sPos=0;tEvents.append(i)\n",
    "    return pd.DatetimeIndex(tEvents)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-71QvPz11RSR"
   },
   "outputs": [],
   "source": [
    "## Daily Volatility Estimator\n",
    "def getDailyVol(close, span0 = 100):\n",
    "    '''\n",
    "    Compute the daily volatility at intraday estimation\n",
    "    applying a span of span0 to an exponentially weighted moving standard deviation\n",
    "    Set profit taking and stop loss limits that are function of the risks involved in a bet\n",
    "    '''\n",
    "    df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))\n",
    "    df0=df0[df0>0]   \n",
    "    df0=(pd.Series(close.index[df0-1], index=close.index[close.shape[0]-df0.shape[0]:]))   \n",
    "    try:\n",
    "        df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily rets\n",
    "    except Exception as e:\n",
    "        print(f'error: {e}\\nplease confirm no duplicate indices')\n",
    "    df0=df0.ewm(span=span0).std().rename('dailyVol')\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6d1AQcAU1RSW"
   },
   "outputs": [],
   "source": [
    "def applyPtSlOnT1(close,events,ptSl,molecule):\n",
    "    # apply stop loss/profit taking, if it takes place before t1 (end of event)\n",
    "    events_=events.loc[molecule]\n",
    "    out=events_[['t1']].copy(deep=True)\n",
    "    if ptSl[0]>0:\n",
    "        pt=ptSl[0]*events_['trgt']\n",
    "    else: \n",
    "        pt=pd.Series(index=events.index) # NaNs\n",
    "    if ptSl[1]>0:\n",
    "        sl=-ptSl[1]*events_['trgt']\n",
    "    else:\n",
    "        sl=pd.Series(index=events.index) # NaNs\n",
    "    \n",
    "    for loc,t1 in events_['t1'].fillna(close.index[-1]).iteritems():\n",
    "        df0=close[loc:t1] # path prices\n",
    "        df0=(df0/close[loc]-1)*events_.at[loc,'side'] # path returns\n",
    "        out.loc[loc,'sl']=df0[df0<sl[loc]].index.min() # earliest stop loss\n",
    "        out.loc[loc,'pt']=df0[df0>pt[loc]].index.min() # earliest profit taking\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0amE3VZ1RSc"
   },
   "outputs": [],
   "source": [
    "def getEvents(close, tEvents, ptSl, trgt, minRet, numThreads, t1=False, side=None):\n",
    "    #1) get target\n",
    "    trgt=trgt.loc[tEvents]\n",
    "    trgt=trgt[trgt>minRet] # minRet\n",
    "    #2) get t1 (max holding period)\n",
    "    if t1 is False:t1=pd.Series(pd.NaT, index=tEvents)\n",
    "    #3) form events object, apply stop loss on t1\n",
    "    if side is None:side_,ptSl_=pd.Series(1.,index=trgt.index), [ptSl[0],ptSl[0]]\n",
    "    else: side_,ptSl_=side.loc[trgt.index],ptSl[:2]\n",
    "    events=(pd.concat({'t1':t1,'trgt':trgt,'side':side_}, axis=1)\n",
    "            .dropna(subset=['trgt']))\n",
    "    df0=mpPandasObj(func=applyPtSlOnT1,pdObj=('molecule',events.index),\n",
    "                    numThreads=numThreads,close=close,events=events,\n",
    "                    ptSl=ptSl_)\n",
    "    events['t1']=df0.dropna(how='all').min(axis=1) # pd.min ignores nan\n",
    "    if side is None:events=events.drop('side',axis=1)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qkv_oVlj1RSf"
   },
   "outputs": [],
   "source": [
    "def addVerticalBarrier(tEvents, close, numDays=1):\n",
    "    t1=close.index.searchsorted(tEvents+pd.Timedelta(days=numDays))\n",
    "    t1=t1[t1<close.shape[0]]\n",
    "    t1=(pd.Series(close.index[t1],index=tEvents[:t1.shape[0]]))\n",
    "    return t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsRnZkuP1RSs"
   },
   "outputs": [],
   "source": [
    "def getBins(events, close):\n",
    "    '''\n",
    "    Compute event's outcome (including side information, if provided). events is a DataFrame where: \n",
    "    —events.index is event's starttime \n",
    "    —events[’t1’] is event's endtime \n",
    "    —events[’trgt’] is event's target \n",
    "    —events[’side’] (optional) implies the algo's position side \n",
    "    Case 1: (’side’ not in events): bin in (-1,1) <—label by price action \n",
    "    Case 2: (’side’ in events): bin in (0,1) <—label by pnl (meta-labeling)\n",
    "    '''\n",
    "    #1) prices aligned with events\n",
    "    events_=events.dropna(subset=['t1'])\n",
    "    px=events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "    px=close.reindex(px,method='bfill')\n",
    "    #2) create out object\n",
    "    out=pd.DataFrame(index=events_.index)\n",
    "    out['ret']=px.loc[events_['t1'].values].values/px.loc[events_.index]-1\n",
    "    if 'side' in events_:out['ret']*=events_['side'] # meta-labeling\n",
    "    out['bin']=np.sign(out['ret'])\n",
    "    if 'side' in events_:out.loc[out['ret']<=0,'bin']=0 # meta-labeling\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WlEKPS-Z1RSw"
   },
   "outputs": [],
   "source": [
    "def dropLabels(events, minPct=.05):\n",
    "    # apply weights, drop labels with insufficient examples\n",
    "    while True:\n",
    "        df0=events['bin'].value_counts(normalize=True)\n",
    "        if df0.min()>minPct or df0.shape[0]<3:break\n",
    "        print('dropped label: ', df0.argmin(),df0.min())\n",
    "        events=events[events['bin']!=df0.argmin()]\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgH8z9PU1RS3"
   },
   "outputs": [],
   "source": [
    "def linParts(numAtoms,numThreads):\n",
    "    # partition of atoms with a single loop\n",
    "    parts=np.linspace(0,numAtoms,min(numThreads,numAtoms)+1)\n",
    "    parts=np.ceil(parts).astype(int)\n",
    "    return parts\n",
    "\n",
    "def nestedParts(numAtoms,numThreads,upperTriang=False):\n",
    "    # partition of atoms with an inner loop\n",
    "    parts,numThreads_=[0],min(numThreads,numAtoms)\n",
    "    for num in range(numThreads_):\n",
    "        part=1+4*(parts[-1]**2+parts[-1]+numAtoms*(numAtoms+1.)/numThreads_)\n",
    "        part=(-1+part**.5)/2.\n",
    "        parts.append(part)\n",
    "    parts=np.round(parts).astype(int)\n",
    "    if upperTriang: # the first rows are heaviest\n",
    "        parts=np.cumsum(np.diff(parts)[::-1])\n",
    "        parts=np.append(np.array([0]),parts)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWAHPyO41RTA"
   },
   "outputs": [],
   "source": [
    "def mpPandasObj(func,pdObj,numThreads=24,mpBatches=1,linMols=True,**kargs):\n",
    "    '''\n",
    "    Parallelize jobs, return a dataframe or series\n",
    "    + func: function to be parallelized. Returns a DataFrame\n",
    "    + pdObj[0]: Name of argument used to pass the molecule\n",
    "    + pdObj[1]: List of atoms that will be grouped into molecules\n",
    "    + kwds: any other argument needed by func\n",
    "    Example: df1=mpPandasObj(func,('molecule',df0.index),24,**kwds)\n",
    "    '''\n",
    "    if linMols:parts=linParts(len(pdObj[1]),numThreads*mpBatches)\n",
    "    else:parts=nestedParts(len(pdObj[1]),numThreads*mpBatches)\n",
    "    \n",
    "    jobs=[]\n",
    "    for i in range(1,len(parts)):\n",
    "        job={pdObj[0]:pdObj[1][parts[i-1]:parts[i]],'func':func}\n",
    "        job.update(kargs)\n",
    "        jobs.append(job)\n",
    "    out=processJobs(jobs,numThreads=numThreads)\n",
    "    if isinstance(out[0],pd.DataFrame):df0=pd.DataFrame()\n",
    "    elif isinstance(out[0],pd.Series):df0=pd.Series()\n",
    "    else:return out\n",
    "    for i in out:df0=df0.append(i)\n",
    "    df0=df0.sort_index()\n",
    "    return df0\n",
    "\n",
    "def processJobs_(jobs):\n",
    "    # Run jobs sequentially, for debugging\n",
    "    out=[]\n",
    "    for job in jobs:\n",
    "        out_=expandCall(job)\n",
    "        out.append(out_)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esbtkifd1RTU"
   },
   "outputs": [],
   "source": [
    "def reportProgress(jobNum,numJobs,time0,task):\n",
    "    # Report progress as asynch jobs are completed\n",
    "    msg=[float(jobNum)/numJobs, (time.time()-time0)/60.]\n",
    "    msg.append(msg[1]*(1/msg[0]-1))\n",
    "    timeStamp=str(dt.datetime.fromtimestamp(time.time()))\n",
    "    msg=timeStamp+' '+str(round(msg[0]*100,2))+'% '+task+' done after '+ \\\n",
    "        str(round(msg[1],2))+' minutes. Remaining '+str(round(msg[2],2))+' minutes.'\n",
    "    if jobNum<numJobs:sys.stderr.write(msg+'\\r')\n",
    "    else:sys.stderr.write(msg+'\\n')\n",
    "    return\n",
    "\n",
    "def processJobs(jobs,task=None,numThreads=24):\n",
    "    # Run in parallel.\n",
    "    # jobs must contain a 'func' callback, for expandCall\n",
    "    if task is None:task=jobs[0]['func'].__name__\n",
    "    pool=mp.Pool(processes=numThreads)\n",
    "    outputs,out,time0=pool.imap_unordered(expandCall,jobs),[],time.time()\n",
    "    # Process asyn output, report progress\n",
    "    for i,out_ in enumerate(outputs,1):\n",
    "        out.append(out_)\n",
    "        reportProgress(i,len(jobs),time0,task)\n",
    "    pool.close();pool.join() # this is needed to prevent memory leaks\n",
    "    return out\n",
    "\n",
    "def expandCall(kargs):\n",
    "    # Expand the arguments of a callback function, kargs['func']\n",
    "    func=kargs['func']\n",
    "    del kargs['func']\n",
    "    out=func(**kargs)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_bars(df,column,m,tick = False):\n",
    "    t = df[column]\n",
    "    ts = 0\n",
    "    idx = []\n",
    "    if tick: # if tick bar\n",
    "        for i,x in enumerate(t):\n",
    "            ts += 1 # each data plus 1\n",
    "            if ts > m:\n",
    "                idx.append(i)\n",
    "                ts = 0\n",
    "    else: # if not tick bar\n",
    "        for i,x in enumerate(t):\n",
    "            ts += x # each data plus volume/dollar volume\n",
    "            if ts > m:\n",
    "                idx.append(i)\n",
    "                ts = 0\n",
    "    return idx\n",
    "\n",
    "def general_bar_df(df,column,m, tick = False):\n",
    "    idx = general_bars(df, column, m, tick)\n",
    "    df = df.iloc[idx].drop_duplicates()\n",
    "    df['dates'] = df['dates'] + pd.to_timedelta(df.groupby('dates').cumcount(), unit='ms')\n",
    "    return df\n",
    "\n",
    "def dollar_bar_df(df,dollar_column,m):\n",
    "    return general_bar_df(df,dollar_column,m)\n",
    "\n",
    "def tick_bars(df,price_column,m):\n",
    "    return general_bars(df,price_column,m, tick = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Import data and run code:\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42eVbqmt1RTf"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('IVE_dollarValue_resampled_1s.parquet')\n",
    "df['dates'] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dollar bars allow us to label our data to retain more information, there are many other options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD0n7E9_1RTn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30859/30859 [00:01<00:00, 20255.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dollar Bars\n",
    "dbars = dollar_bar_df(df,'dv',1_000_000).drop_duplicates().dropna() # Measure our data\n",
    "close = dbars.price.copy()\n",
    "dailyVol = getDailyVol(close) # Measure our risk\n",
    "tEvents = getTEvents(close, h = dailyVol.mean()) #Given a time series and a threshold(profit/stop order), allows for event based sampling. Think bollinger banads but not activated when overs around event threshold, take into account a time frame. \n",
    "t1 = addVerticalBarrier(tEvents,close) # Vertical barrier - max length of time bet will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZ7cJDuk1RUj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 14:05:18.611525 100.0% applyPtSlOnT1 done after 0.03 minutes. Remaining 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Triple-barrier:\n",
    "\n",
    "# Target Series\n",
    "ptSl = [1,1] # factor that multiplies trgt to set the width of the upper barrier. If 0, there will not be an upper barrier\n",
    "target = dailyVol # Risk tolerance\n",
    "minRet = 0.01 # minimum acceptable return required for running triple barrier method\n",
    "events = getEvents(close,tEvents, ptSl, target, minRet,1, t1=t1) # finds the time of the first barrier touch\n",
    "labels = getBins(events, close) # getBins - generate labels\n",
    "clean_labels = dropLabels(labels) # drop under-populated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Or_6fJGt1RVG",
    "outputId": "650005e3-9a50-46ad-e59a-92946ead0696"
   },
   "outputs": [],
   "source": [
    "# Moving Average Crossover Strategy. - Model gives side but not size of bet. Aka our primary model\n",
    "fast_window = 3\n",
    "slow_window = 7\n",
    "close_df = (pd.DataFrame().assign(price = close).assign(fast=close.ewm(fast_window).mean()).assign(slow =close.ewm(slow_window).mean()))\n",
    "\n",
    "def get_up_cross(df):\n",
    "    crit1 = df.fast.shift(1) < df.slow.shift(1)\n",
    "    crit2 = df.fast > df.slow\n",
    "    return df.fast[(crit1)&(crit2)]\n",
    "\n",
    "def get_down_cross(df):\n",
    "    crit1 = df.fast.shift(1) > df.slow.shift(1)\n",
    "    crit2 = df.fast < df.slow\n",
    "    return df.slow[(crit1) & (crit2)]\n",
    "\n",
    "up = get_up_cross(close_df)\n",
    "down = get_down_cross(close_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLtgqRGh1RVP",
    "outputId": "a2c04119-0f9b-418f-dac9-7733d9e8c382"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 14:05:21.594629 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Meta-labels: our secondary model\n",
    "# Trading signal\n",
    "side_up = pd.Series(1,index = up.index)\n",
    "side_down = pd.Series(-1, index = down.index)\n",
    "side = pd.concat([side_up, side_down]).sort_index()\n",
    "minRet = .01 # minimum acceptable return required for running triple barrier method\n",
    "ptSl = [1,2] # factor that multiplies trgt to set the width of the upper barrier. If 0, there will not be an upper barrier\n",
    "ma_events = getEvents(close, tEvents, ptSl, target, minRet, 1, t1 = t1, side = side) # finds the time of the first barrier touch for simple moving average trend following strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rf3m5qQQ1RVX"
   },
   "outputs": [],
   "source": [
    "ma_side= ma_events.dropna().side # Drop nulls, get sides\n",
    "ma_bins = getBins(ma_events,close).dropna()  # getBins - generate labels\n",
    "Xx = pd.merge_asof(ma_bins, side.to_frame().rename(columns={0:'side'}), left_index=True, right_index=True, direction='forward') # Merge side with labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14388169456817856"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xx.ret.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igZ4fmpN1RWn"
   },
   "source": [
    "Train Random Forest to choose to trade or not {0,1} since undelying model, Cross SMA, has decided the side, {-1,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xomQ-YcW1RW3",
    "outputId": "1e13397d-4656-498a-e51c-066db4beabfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        24\n",
      "         1.0       0.53      1.00      0.69        27\n",
      "\n",
      "    accuracy                           0.53        51\n",
      "   macro avg       0.26      0.50      0.35        51\n",
      "weighted avg       0.28      0.53      0.37        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ma_side.values.reshape(-1,1) # sides\n",
    "y = ma_bins.bin.values # Labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "n_estimator = 10000\n",
    "rf = RandomForestClassifier(max_depth=2, n_estimators=n_estimator, criterion='entropy', random_state=42)\n",
    "rf.fit(X_train, y_train);\n",
    "\n",
    "# The random forest model by itself\n",
    "y_pred_rf = rf.predict_proba(X_test)[:,1]\n",
    "y_pred = rf.predict(X_test)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our results aren't necessarily ideal, we view the process of breaking down a typical machine learning model, and provide a preview of the current state of affairs for explainability in applications of machine learning in finance. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Labeling and MetaLabeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
